{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://centralindia.api.cognitive.microsoft.com/\"\n",
        "\n",
        "try:\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    print(\"Success\")\n",
        "  else:\n",
        "    print(f\"Failure: {response.status_code}\")\n",
        "except request.exceptions.RequestException as e:\n",
        "  print(\"fError: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oaqih68qN4Zq",
        "outputId": "0476ffb8-ada5-4273-93b4-9cddc41a49a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCl6eN3ZSWlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897a8b21-070c-406e-bb24-0e12c6df2798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.37.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.21.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n",
            "Collecting azure-core>=1.28.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.30.2-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (42.0.8)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Downloading streamlit-1.37.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.21.0-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: azure-common, watchdog, tenacity, smmap, isodate, pydeck, gitdb, azure-core, gitpython, azure-storage-blob, azure-ai-textanalytics, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.30.2 azure-storage-blob-12.21.0 gitdb-4.0.11 gitpython-3.1.43 isodate-0.6.1 pydeck-0.9.1 smmap-5.0.1 streamlit-1.37.0 tenacity-8.5.0 watchdog-4.0.1\n",
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "Collecting openai\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit azure-storage-blob azure-ai-textanalytics requests\n",
        "!npm install -g localtunnel\n",
        "!pip install openai\n",
        "!pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Next steps froom midterm presentation: Measure model;s efficiency at handling multiple language audio. Furthermore, see it scapabilites regarding\n",
        "#audio with background noise, audio with multiple speakers, etc.\n",
        "\n",
        "#Implementation 1: Test multiple language audio. Find clips and then test the efficency with comparasions vs. other transcription services\n",
        "#Implementation 2: Test multiple speakers audio. Find clips and then test the efficency with comparasions vs. other transcription services\n",
        "#Implementation 3: Test multiple langauge and multiple speakers audio. Find clips and then test the efficency with comparasions vs. other transcription services\n",
        "#Implementation 4: Test background noise. Find clips and then test the efficency with comparasions vs. other transcription services\n",
        "#Implementaiton 5: Test multiple language, multiple spakers audio. Find clips and then test the efficency with comparasions vs. other transcription services\n",
        "#Other transcription services to try: Online sources, test whisper models as well (Chetana requested).  Confirm with Chetana what she wants the end product/use case to be\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Provided values\n",
        "connection_string =\n",
        "container_name =\n",
        "blob_name =\n",
        "\n",
        "# Azure Text Analytics credentials\n",
        "text_analytics_key =\n",
        "text_analytics_endpoint =\n",
        "\n",
        "# Azure Speech to Text API\n",
        "subscription_key =\n",
        "service_region =\n",
        "transcription_locale =\n",
        "\n",
        "# The SAS token should be correctly generated and added to the URL\n",
        "sas_token =\n",
        "content_container_url =\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"File Transcription and Keyword Extraction\")\n",
        "\n",
        "#Step 0.5: Choose the language you want your keywords to be in\n",
        "supported_languages = {\n",
        "    \"English\": \"en\",\n",
        "    \"Hindi\": \"hi\",\n",
        "    \"Marathi\": \"mr\",\n",
        "    \"Tamil\": \"ta\",\n",
        "    \"Mandarin\": \"zh-Hans\",\n",
        "    \"Spanish\": \"es\",\n",
        "    \"French\": \"fr\",\n",
        "    \"Arabic\": \"ar\",\n",
        "    \"Russian\": \"ru\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# If you want to use this in a Streamlit selectbox:\n",
        "import streamlit as st\n",
        "\n",
        "language_key = st.selectbox(\n",
        "    \"Choose the language you'd like your keywords to be in\", [\"English\", \"Hindi\", \"Marathi\", \"Tamil\", \"Mandarin\", \"Spanish\", \"French\", \"Arabic\", \"Russian\"])\n",
        "\n",
        "selected_language = supported_languages[language_key]\n",
        "\n",
        "# Step 1: Upload the file\n",
        "uploaded_file = st.file_uploader(\"Choose a file...\", type=[\"mp4\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Step 2: Upload the file to Azure Blob Storage\n",
        "    with st.spinner(\"Uploading file to Azure Blob Storage...\"):\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "        # Save the uploaded file to a temporary location\n",
        "        temp_file_path = f\"/tmp/{uploaded_file.name}\"\n",
        "        with open(temp_file_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "        # Upload the file to Azure Blob Storage\n",
        "        with open(temp_file_path, \"rb\") as data:\n",
        "            container_client.upload_blob(name=blob_name, data=data, overwrite=True)\n",
        "\n",
        "    st.success(f\"File {uploaded_file.name} uploaded successfully.\")\n",
        "\n",
        "    # Step 3: Transcribe the audio\n",
        "    # Step 3: Transcribe the audio\n",
        "with st.spinner(\"Starting transcription...\"):\n",
        "    transcription_payload = {\n",
        "        \"displayName\": uploaded_file.name,\n",
        "        \"description\": \"Speech Studio Batch speech to text\",\n",
        "        \"locale\": transcription_locale,\n",
        "        \"contentContainerUrl\": content_container_url,\n",
        "        \"properties\": {\n",
        "            \"wordLevelTimestampsEnabled\": False,\n",
        "            \"displayFormWordLevelTimestampsEnabled\": True,\n",
        "            \"diarizationEnabled\": True,\n",
        "            \"languageIdentification\": {\n",
        "                \"candidateLocales\": [\n",
        "                    \"en-us\",\n",
        "                    \"en-IN\",\n",
        "                    \"hi-IN\",\n",
        "                    \"ta-IN\",\n",
        "                    \"zh-CN\",\n",
        "                    \"es-ES\",\n",
        "                    \"fr-FR\",\n",
        "                    \"ar-EG\",\n",
        "                    \"ru-RU\"\n",
        "                ]\n",
        "            },\n",
        "            \"diarization\": {\n",
        "                \"speakers\": {\n",
        "                    \"minCount\": 1,\n",
        "                    \"maxCount\": 10\n",
        "                }\n",
        "            },\n",
        "            \"punctuationMode\": \"DictatedAndAutomatic\",\n",
        "            \"profanityFilterMode\": \"Masked\"\n",
        "        },\n",
        "        \"customProperties\": {}\n",
        "    }\n",
        "\n",
        "    transcription_response = requests.post(\n",
        "        f\"https://{service_region}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions\",\n",
        "        headers={\"Ocp-Apim-Subscription-Key\": subscription_key, \"Content-Type\": \"application/json\"},\n",
        "        data=json.dumps(transcription_payload)\n",
        "    )\n",
        "\n",
        "    if transcription_response.status_code in [200, 201]:\n",
        "        transcription_response_data = transcription_response.json()\n",
        "        # st.write(\"Transcription Response:\", transcription_response_data)  # Debugging statement\n",
        "        self_url = transcription_response_data.get(\"self\", \"\")\n",
        "        transcription_id = self_url.split(\"/\")[-1]\n",
        "        st.write(f\"Transcription ID: {transcription_id}\")\n",
        "        # Save the transcription ID in Streamlit session state\n",
        "        st.session_state.transcription_id = transcription_id\n",
        "        st.session_state.transcription_status = \"Running\"\n",
        "        st.success(\"Transcription has started. Please check back later to retrieve the result.\")\n",
        "    else:\n",
        "        st.error(f\"Failed to start transcription. Status code: {transcription_response.status_code}\")\n",
        "        st.write(\"Response:\", transcription_response.text)\n",
        "\n",
        "\n",
        "# Step 4: Polling for Transcription Status and Retrieve Result\n",
        "if 'transcription_id' in st.session_state:\n",
        "    transcription_id = st.session_state.transcription_id\n",
        "\n",
        "    # Polling for transcription status\n",
        "    if st.session_state.transcription_status == \"Running\":\n",
        "        with st.spinner(\"Checking transcription status...\"):\n",
        "            while True:\n",
        "                status_response = requests.get(\n",
        "                    f\"https://{service_region}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions/{transcription_id}\",\n",
        "                    headers={\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
        "                )\n",
        "                status_data = status_response.json()\n",
        "                # st.write(\"Status Response:\", status_data)  # Debugging statement\n",
        "\n",
        "                if status_data[\"status\"] == \"Succeeded\":\n",
        "                    st.session_state.transcription_status = \"Succeeded\"\n",
        "                    break\n",
        "                elif status_data[\"status\"] == \"Failed\":\n",
        "                    st.session_state.transcription_status = \"Failed\"\n",
        "                    break\n",
        "                else:\n",
        "                    st.session_state.transcription_status = \"Running\"\n",
        "                    time.sleep(100)  # Polling interval\n",
        "\n",
        "    if st.session_state.transcription_status == \"Succeeded\":\n",
        "        with st.spinner(\"Retrieving transcription result...\"):\n",
        "            files_uri = f\"https://{service_region}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions/{transcription_id}/files\"\n",
        "\n",
        "            response = requests.get(files_uri, headers={\"Ocp-Apim-Subscription-Key\": subscription_key})\n",
        "            response_data = response.json()\n",
        "\n",
        "           # st.write(\"Files URI Response:\", response_data)  # Debugging statement to check the response data\n",
        "\n",
        "            content_urls = [file['links']['contentUrl'] for file in response_data['values'] if file['kind'] == 'Transcription']\n",
        "\n",
        "            textString = \"\"\n",
        "            for content_url in content_urls:\n",
        "                content_response = requests.get(content_url)\n",
        "                content_data = content_response.json()\n",
        "                lexical_phrases = [phrase['lexical'] for phrase in content_data.get('combinedRecognizedPhrases', [])]\n",
        "\n",
        "                for lexical in lexical_phrases:\n",
        "                    textString += lexical + \" \"  # Add a space to separate phrases\n",
        "\n",
        "        st.success(\"Transcription completed.\")\n",
        "        st.write(\"Transcribed Text:\")\n",
        "        st.write(textString)  # Display the transcribed text for debugging\n",
        "\n",
        "\n",
        "\n",
        "#Step 4.5: Convert the transcription into the selected language\n",
        "\n",
        "        # Set up environment variables\n",
        "language_key =\n",
        "language_endpoint =\n",
        "translator_key =\n",
        "translator_endpoint =\n",
        "location =\n",
        "\n",
        "# Authenticate the Text Analytics client\n",
        "def authenticate_text_analytics_client():\n",
        "    ta_credential = AzureKeyCredential(language_key)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=language_endpoint,\n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "\n",
        "text_analytics_client = authenticate_text_analytics_client()\n",
        "\n",
        "# Segment text into chunks of 5 words each\n",
        "def segment_text(text, chunk_size=5):\n",
        "    words = text.split()\n",
        "    segments = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return segments\n",
        "\n",
        "# Detect language for each text segment\n",
        "def detect_languages(client, text_segments):\n",
        "    try:\n",
        "        responses = client.detect_language(documents=text_segments, country_hint='us')\n",
        "        detected_languages = [response.primary_language.iso6391_name for response in responses]\n",
        "        return detected_languages\n",
        "    except Exception as err:\n",
        "        print(f\"Encountered exception: {err}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "# Translate each segment into the selected language\n",
        "def translate_segments(segments, detected_languages, to_language):\n",
        "    path = '/translate'\n",
        "    constructed_url = translator_endpoint + path\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': translator_key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "    }\n",
        "    translated_segments = []\n",
        "    for i, segment in enumerate(segments):\n",
        "        body = [{'text': segment}]\n",
        "        params = {\n",
        "            'api-version': '3.0',\n",
        "            'from': detected_languages[i],\n",
        "            'to': to_language\n",
        "        }\n",
        "        response = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "        response_json = response.json()\n",
        "        print(f\"Response JSON for segment {i}: {json.dumps(response_json, indent=4, ensure_ascii=False)}\")  # Debug print\n",
        "\n",
        "        try:\n",
        "            # Accessing the translated text from the JSON response\n",
        "            translated_text = response_json[0]['translations'][0]['text']\n",
        "            translated_segments.append(translated_text)\n",
        "        except (IndexError, KeyError) as e:\n",
        "            print(f\"Error accessing translation for segment {i}: {e}\")\n",
        "            translated_segments.append(segment)  # Fallback to original segment in case of error\n",
        "    return translated_segments\n",
        "\n",
        "\n",
        "\n",
        "# Combine translated segments into a single string\n",
        "def combine_translations(translated_segments):\n",
        "    return ' '.join(translated_segments)\n",
        "\n",
        "# Sample multilingual text\n",
        "text = textString\n",
        "\n",
        "# Segment text into chunks of 5 words each\n",
        "text_segments = segment_text(text, chunk_size=5)\n",
        "\n",
        "# Detect languages\n",
        "detected_languages = detect_languages(text_analytics_client, text_segments)\n",
        "print(detected_languages)\n",
        "\n",
        "# Translate segments into the selected language (e.g., English 'en')\n",
        "selected_language = 'en'\n",
        "translated_segments = translate_segments(text_segments, detected_languages, selected_language)\n",
        "\n",
        "# Combine translated segments into a single string\n",
        "final_translated_text = combine_translations(translated_segments)\n",
        "\n",
        "st.write(\"Translated Text:\")\n",
        "st.write(final_translated_text)  # Display the transcribed text for debugging\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Extract keywords using Azure Text Analytics\n",
        "with st.spinner(\"Extracting keywords...\"):\n",
        "\n",
        "    def extract_key_phrases(text):\n",
        "        text_analytics_client = TextAnalyticsClient(endpoint=text_analytics_endpoint, credential=AzureKeyCredential(text_analytics_key))\n",
        "        documents = [text]\n",
        "        response = text_analytics_client.extract_key_phrases(documents=documents)[0]\n",
        "\n",
        "        if not response.is_error:\n",
        "            return response.key_phrases\n",
        "        else:\n",
        "            raise Exception(\"Error in extracting key phrases: \" + response.error.message)\n",
        "\n",
        "    # Assuming textString contains the transcribed text\n",
        "    try:\n",
        "        key_phrases = extract_key_phrases(final_translated_text)\n",
        "        st.success(\"Keyword extraction completed.\")\n",
        "        st.write(\"Key phrases:\")\n",
        "        for phrase in key_phrases:\n",
        "\n",
        "            st.write(f\"- {phrase}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to extract key phrases: {str(e)}\")\n",
        "\n",
        "\n",
        "def translate_keyword(keyword, to_language):\n",
        "    path = '/translate'\n",
        "    constructed_url = translator_endpoint + path\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': translator_key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "    }\n",
        "    body = [{'text': keyword}]\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'to': to_language\n",
        "    }\n",
        "    response = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "    response_json = response.json()\n",
        "    try:\n",
        "        translated_text = response_json[0]['translations'][0]['text']\n",
        "        return translated_text\n",
        "    except (IndexError, KeyError) as e:\n",
        "        print(f\"Error accessing translation for text '{text}': {e}\")\n",
        "        return text  # Fallback to original text in case of error\n",
        "\n",
        "with st.spinner(\"Re-translating keywords\"):\n",
        "    st.write(\"Newly translated key phrases:\")\n",
        "    for phrase in key_phrases:\n",
        "      translated_phrase = translate_keyword(phrase, selected_language)\n",
        "      st.write(f\"- {translated_phrase}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####### OpenAI STT time\n",
        "\n",
        "\n",
        "client = OpenAI(api_key = )\n",
        "\n",
        "with st.spinner(\"OpenAi STT process started:\"):\n",
        "\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "      model=\"whisper-1\",\n",
        "      file=uploaded_file\n",
        "    )\n",
        "# Print the transcribed text\n",
        "st.write(\"OpenAI Whisper Output\")\n",
        "st.write(transcription.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tM215XEwSbCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46ddc5a-5c47-4f87-85ff-049fda8b9dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "bL2YGyIZSf-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Create a tunnel with LocalTunnel\n",
        "tunnel_process = subprocess.Popen(['npx', 'localtunnel', '--port', '8501'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Retrieve the public URL from the output\n",
        "public_url = None\n",
        "for line in iter(tunnel_process.stdout.readline, b''):\n",
        "    decoded_line = line.decode('utf-8').strip()\n",
        "    if 'your url is' in decoded_line:\n",
        "        public_url = decoded_line.split(' ')[-1]\n",
        "        break\n",
        "\n",
        "# Get the tunnel password\n",
        "password = subprocess.check_output(['curl', 'https://loca.lt/mytunnelpassword']).decode('utf-8').strip()\n",
        "\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(f\"Tunnel Password: {password}\")\n"
      ],
      "metadata": {
        "id": "kBo0ei14SiJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf8292c-31f3-4d71-9bf6-4ce9e5d33770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://blue-dots-brake.loca.lt\n",
            "Tunnel Password: 34.80.60.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Git\n",
        "!git config --global user.name \"blu3542\"\n",
        "!git config --global user.email \"b.lu789321@gmail.com\"\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/blu3542/InfoSys\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd InfoSys\n",
        "\n",
        "# Save the Colab notebook to the current directory (if it was saved elsewhere)\n",
        "!mv /content/InfoSysStreamLitApplication.ipynb .\n",
        "\n",
        "# Add the notebook to the staging area\n",
        "!git add InfoSysStreamLitApplication.ipynb\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"Added Google Colab notebook\"\n",
        "\n",
        "# Push the changes to GitHub\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPr48ZE4h18X",
        "outputId": "a264b0d5-3fa4-4ee9-8703-8e46d08f4707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'InfoSys'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (12/12), 116.43 KiB | 3.88 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/InfoSys\n",
            "mv: cannot stat '/content/InfoSysStreamLitApplication.ipynb': No such file or directory\n",
            "fatal: pathspec 'InfoSysStreamLitApplication.ipynb' did not match any files\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    }
  ]
}